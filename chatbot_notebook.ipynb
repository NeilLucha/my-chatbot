{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cd2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neillucha/projects/my-chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "import evaluate\n",
    "import accelerate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005d682d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9f502",
   "metadata": {},
   "source": [
    "# Response Generating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb30b865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('GPU name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767d3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a response\n",
    "def generate_response(input_sequence, max_length=100):\n",
    "    encoded_input = tokenizer(input_sequence + tokenizer.eos_token, return_tensors='pt').to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    encoded_response = model.generate(\n",
    "        **encoded_input,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        # attention_mask=encoded_input['attention_mask'] #required for some models  like DialoGPT but not for T5\n",
    "        # do_sample=False,  # Set to False for deterministic output\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tokenizer.decode(encoded_response[0].cpu(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7efbce",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83b5f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(no )'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "sequence = \"You are a knowledgable and factual assistant: What comes after 5?\"\n",
    "generate_response(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180b689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a genesis'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response('Does anyonw know what enegy from mass anniihilation means?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4b6453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nuclear energy is a renewable resource'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"What are the advantages and disadvantages of nuclear energy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ad6b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Especially for people with autism, we have to provide them with the ability to do the right thing.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Please answer the following question: What do you think about the benefit of Artificial Intelligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4616c94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reinforcement'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"System: You are a helpful and factual assistant.\\nUser's Question: Define the term 'reinforcement'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a11bac",
   "metadata": {},
   "source": [
    "# Implementing Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac4ebc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    # 'You are a knowledgable and factual assistant'\n",
    "]\n",
    "\n",
    "def format_chat_history(history, history_length=5):\n",
    "    return '\\n'.join(history[-history_length*2:] if len(history) > history_length*2 else history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ae498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chat_history(input_sequence):\n",
    "    chat_history.append('user: '+input_sequence)\n",
    "    formatted = format_chat_history(chat_history)\n",
    "    print(f\"Formatted chat history: {formatted}\")\n",
    "    response = generate_response(formatted+'\\nbot: ')\n",
    "    chat_history.append('bot: '+response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688735ea",
   "metadata": {},
   "source": [
    "# Chat History Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c03cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat history: user: Translate to German: 'What is the capital of France?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"user: Translate to German: 'What is the capital of France?'\",\n",
       " \"bot: 'What is the capital of France?'\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = \"Translate to German: 'What is the capital of France?'\"\n",
    "add_to_chat_history(prompt1)\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ca12fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat history: user: Translate to German: 'What is the capital of France?'\n",
      "bot: 'What is the capital of France?'\n",
      "user: Which city is the most populated in the United States?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"user: Translate to German: 'What is the capital of France?'\",\n",
       " \"bot: 'What is the capital of France?'\",\n",
       " 'user: Which city is the most populated in the United States?',\n",
       " \"bot: 'Walt – die Hauptstadt der Stadt?' bot: 'Walt – die Hauptstadt der Stadt?'\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = 'Which city is the most populated in the United States?'\n",
    "add_to_chat_history(prompt2)\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5897a",
   "metadata": {},
   "source": [
    "# Fine Tuning using Yahoo QA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bddb4",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fab2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yahoo_answers_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8360b1",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de88cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65f0e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'nbestanswers', 'main_category'],\n",
       "        num_rows: 78625\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'nbestanswers', 'main_category'],\n",
       "        num_rows: 8737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57633e39",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Answer the question: '\n",
    "\n",
    "def preprocess(input):\n",
    "    prefixed_input = [prefix + question for question in input['question']]\n",
    "    model_input = tokenizer(prefixed_input, truncation=True, max_length=128)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():-\n",
    "        labels = tokenizer(input['answer'], truncation=True, max_length=512)\n",
    "        \n",
    "    model_input['labels'] = labels['input_ids']\n",
    "    return model_input    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64e4214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/78625 [00:00<?, ? examples/s]/home/neillucha/projects/my-chatbot/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 78625/78625 [00:06<00:00, 11521.81 examples/s]\n",
      "Map: 100%|██████████| 8737/8737 [00:00<00:00, 14579.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b747d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question: why does reading in the dark make your eyes worse?</s>\n",
      "If there isn't enough light in the room for your eyes to focus correctly and see what you're reading, your eyes have to strain themselves to try to see.. . Prolonged eye strain can ruin your sight, or make it worse than it was.</s>\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized_dataset['train'][0]\n",
    "print(tokenizer.decode(sample['input_ids']))\n",
    "print(tokenizer.decode(sample['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84e230c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3158900', 'question': 'why do people still stuff from the store?', 'answer': \"There could be a number of reasons why people steal, it could be peer pressure, it could be they are homeless and hungry, they might have a drug problem, or they might be sick, some people have an illness where they might need help from a professional and, then there are the ones who actually do it cause they like the thrill, you know the thrill seekers, doing it just to see if they can get away with it, Those are a few of the reasons why I think people do it. I wish there was something we could do as human beings looking out for each other, Oh my bad we don't....\", 'nbestanswers': [\"it really depends they could be doingg it to show off or they mite be doin it cause they don't have the money for it and maybe they really needed the things.\", \"People steal for a lot of different reasons.  Most of them do it because they don't have the money to buy the merchandise. Others do it because they are angry at the store or the workers for something that was said or done to them.  Then there are the people who just don't need a reason & when they get caught, they can't even think of a reason why they did it, they usually suffer from some form of an addiction which drives them to steal.\", 'Do you mean steal?', \"There could be a number of reasons why people steal, it could be peer pressure, it could be they are homeless and hungry, they might have a drug problem, or they might be sick, some people have an illness where they might need help from a professional and, then there are the ones who actually do it cause they like the thrill, you know the thrill seekers, doing it just to see if they can get away with it, Those are a few of the reasons why I think people do it. I wish there was something we could do as human beings looking out for each other, Oh my bad we don't....\", 'Well if they are STEALING stuff from the store it is probably because some one put them up to it or they are trying to get attention.'], 'main_category': 'Local Businesses', 'input_ids': [11801, 8, 822, 10, 572, 103, 151, 341, 2005, 45, 8, 1078, 58, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [290, 228, 36, 3, 9, 381, 13, 2081, 572, 151, 11332, 6, 34, 228, 36, 11409, 1666, 6, 34, 228, 36, 79, 33, 15876, 11, 13802, 6, 79, 429, 43, 3, 9, 2672, 682, 6, 42, 79, 429, 36, 6802, 6, 128, 151, 43, 46, 7095, 213, 79, 429, 174, 199, 45, 3, 9, 771, 11, 6, 258, 132, 33, 8, 2102, 113, 700, 103, 34, 1137, 79, 114, 8, 22695, 6, 25, 214, 8, 22695, 25321, 6, 692, 34, 131, 12, 217, 3, 99, 79, 54, 129, 550, 28, 34, 6, 3, 3405, 33, 3, 9, 360, 13, 8, 2081, 572, 27, 317, 151, 103, 34, 5, 27, 1663, 132, 47, 424, 62, 228, 103, 38, 936, 271, 7, 479, 91, 21, 284, 119, 6, 3359, 82, 1282, 62, 278, 31, 17, 2824, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(tokenized_dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e156193",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb63ae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1387 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 1387\n",
      "95th percentile: 120.0\n"
     ]
    }
   ],
   "source": [
    "# Checking if the generation_max_length is appropriate\n",
    "\n",
    "label_lengths = [len(tokenizer.encode(ans)) for ans in dataset['train']['answer']]\n",
    "import numpy as np\n",
    "print(f\"Max: {max(label_lengths)}\")\n",
    "print(f\"95th percentile: {np.percentile(label_lengths, 95)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "109b606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=False,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48704912",
   "metadata": {},
   "source": [
    "## ROUGE Metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9041eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/neillucha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84912073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize without nltk\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9523b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean and filter\n",
    "    cleaned_preds = []\n",
    "    cleaned_labels = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        pred = pred.strip()\n",
    "        label = label.strip()\n",
    "        if pred and label and len(pred) < 1000 and len(label) < 1000:\n",
    "            cleaned_preds.append(\"\\n\".join(sent_tokenize(pred)))\n",
    "            cleaned_labels.append(\"\\n\".join(sent_tokenize(label)))\n",
    "\n",
    "    if not cleaned_preds or not cleaned_labels:\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "\n",
    "    result = rouge.compute(predictions=cleaned_preds, references=cleaned_labels, use_stemmer=True)\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1aacece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(63.6364), 'rouge2': np.float64(20.0), 'rougeL': np.float64(54.5455), 'rougeLsum': np.float64(54.5455)}\n"
     ]
    }
   ],
   "source": [
    "# Checking if compute_rouge works\n",
    "\n",
    "sample_preds = [\"Artificial Intelligence is the simulation of human intelligence by machines.\"]\n",
    "sample_labels = [\"Artificial Intelligence refers to the ability of machines to mimic human intelligence.\"]\n",
    "\n",
    "preds = tokenizer(sample_preds, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
    "labels = tokenizer(sample_labels, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
    "\n",
    "labels = np.where(labels == tokenizer.pad_token_id, -100, labels)\n",
    "\n",
    "results = compute_rouge((preds, labels))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f07572",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd4bcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02d9ce",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf9b592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1149/1879313939.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_rouge,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c1273",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35a13272",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7af5ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58971' max='58971' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58971/58971 4:05:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.665200</td>\n",
       "      <td>3.439815</td>\n",
       "      <td>14.340500</td>\n",
       "      <td>2.333700</td>\n",
       "      <td>12.071500</td>\n",
       "      <td>12.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.612600</td>\n",
       "      <td>3.415636</td>\n",
       "      <td>14.526100</td>\n",
       "      <td>2.375200</td>\n",
       "      <td>12.152000</td>\n",
       "      <td>12.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.578600</td>\n",
       "      <td>3.407772</td>\n",
       "      <td>14.413200</td>\n",
       "      <td>2.343600</td>\n",
       "      <td>12.035600</td>\n",
       "      <td>12.448200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=58971, training_loss=3.6384050967914843, metrics={'train_runtime': 14735.4218, 'train_samples_per_second': 16.007, 'train_steps_per_second': 4.002, 'total_flos': 2107544852170752.0, 'train_loss': 3.6384050967914843, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35ff8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = tokenized_dataset['train'][0]\n",
    "\n",
    "# input_ids = torch.tensor([sample['input_ids']])\n",
    "# labels = torch.tensor([sample['labels']])\n",
    "\n",
    "# # Move to GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# input_ids = input_ids.to(device)\n",
    "# labels = labels.to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids=input_ids, labels=labels)\n",
    "#     print(\"Loss:\", output.loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a715b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Labels:\", labels)\n",
    "# print(\"Are all labels -100?\", (labels == -100).all())\n",
    "# print(\"Any NaNs?\", torch.isnan(labels).any())\n",
    "# print(\"Min label:\", labels.min())\n",
    "# print(\"Max label:\", labels.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "728386f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import default_data_collator\n",
    "\n",
    "# # Filter the dataset to include only fields needed for the model\n",
    "# processed_dataset = tokenized_dataset[\"train\"].remove_columns(\n",
    "#     [\"question\", \"answer\", \"nbestanswers\", \"main_category\", \"id\"]\n",
    "# )\n",
    "\n",
    "# # Now use DataLoader safely\n",
    "# dataloader = DataLoader(\n",
    "#     processed_dataset,\n",
    "#     batch_size=1,\n",
    "#     collate_fn=default_data_collator\n",
    "# )\n",
    "\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "# # Move tensors to correct device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# input_ids = batch[\"input_ids\"].to(device)\n",
    "# attention_mask = batch[\"attention_mask\"].to(device)\n",
    "# labels = batch[\"labels\"].to(device)\n",
    "\n",
    "# # Forward pass\n",
    "# model.to(device)\n",
    "# outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "# print(\"Loss:\", outputs.loss.item())\n",
    "# print(\"Logits shape:\", outputs.logits.shape)\n",
    "# print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f57af487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"All -100?:\", torch.all(labels == -100))\n",
    "# print(\"Any NaNs in labels?:\", torch.isnan(labels).any())\n",
    "# print(\"Unique label values:\", torch.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef3738c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Logits contain NaNs?:\", torch.isnan(outputs.logits).any())\n",
    "# print(\"Logits contain infs?:\", torch.isinf(outputs.logits).any())\n",
    "# print(\"Max logit:\", torch.max(outputs.logits))\n",
    "# print(\"Min logit:\", torch.min(outputs.logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee0bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
